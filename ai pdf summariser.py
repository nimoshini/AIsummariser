# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yQGCdDnw1Yg_XAMIMedBBJmvPUp-xnNH
"""



import gradio as gr
import pdfplumber
import requests
import textwrap
import os

# Groq API Key (Replace with your actual key)
GROQ_API_KEY = "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"
GROQ_ENDPOINT = "https://api.groq.com/openai/v1/chat/completions"

# Define the models
MIXTRAL_MODEL = "mixtral-8x7b-32768"
LLAMA_MODEL = "llama-3.3-70b-versatile"

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

# Function to interact with Groq API
def generate_summary(text, model):
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a research summarization assistant."},
            {"role": "user", "content": f"Summarize the following research paper:\n\n{text}"}
        ],
        "max_tokens": 1024
    }

    response = requests.post(GROQ_ENDPOINT, json=payload, headers=headers)

    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        return f"Error: {response.json()}"

# Function to handle large text by chunking
def chunk_text(text, chunk_size=4500):  # Keep it within 5000 token limit
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunks.append(" ".join(words[i:i+chunk_size]))
    return chunks

# Main function
def summarize_research_paper(pdf_file, summary_type):
    if pdf_file is None:
        return "Please upload a PDF file."

    pdf_path = pdf_file.name
    print("Extracting text from PDF...")
    research_text = extract_text_from_pdf(pdf_path)

    print("Chunking text to fit within API limits...")
    text_chunks = chunk_text(research_text)
    summary = ""

    if summary_type == "Detailed Summary (Mixtral-8x7B)":
        print("Generating detailed summary...")
        for chunk in text_chunks:
            summary += generate_summary(chunk, MIXTRAL_MODEL) + "\n\n"
    else:
        print("Generating key points summary...")
        detailed_summary = ""
        for chunk in text_chunks:
            detailed_summary += generate_summary(chunk, MIXTRAL_MODEL) + "\n\n"
        summary = generate_summary(detailed_summary, LLAMA_MODEL)

    return summary

# Gradio UI
def gradio_interface(pdf_file, summary_type):
    return summarize_research_paper(pdf_file, summary_type)

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ§  AI-Powered PDF Summarizer")
    gr.Markdown("Upload a research paper and choose how you want it summarized.")

    pdf_input = gr.File(label="Upload Research Paper (PDF)")
    summary_type = gr.Radio([
        "Detailed Summary (Mixtral-8x7B)",
        "Key Points Summary (LLaMA-3.3-70B)"
    ], label="Choose Summary Type")
    submit_button = gr.Button("Generate Summary")
    output_text = gr.Textbox(label="Generated Summary", lines=10)

    submit_button.click(fn=gradio_interface, inputs=[pdf_input, summary_type], outputs=output_text)

demo.launch()